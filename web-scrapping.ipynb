{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping - Global Forest Watch data\n",
    "- We create the output folder in case it does not exists in the \"Downloads folder\"\n",
    "- This script iterates over the dataset repository\n",
    "- Gets the link headers \"xxxx.tif\"\n",
    "- Since they don't contain all the strcuture of the sentence, we build the input link with the link header\n",
    "- Pointing to the download created folder we download them one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website to scrape\n",
    "url = \"https://storage.googleapis.com/earthenginepartners-hansen/GFC-2022-v1.10/treecover2000.txt\"\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# Find all links in the webpage\n",
    "links = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder inside \"Downloads\" to store downloaded files\n",
    "def set_output_folder(output_folder_name):\n",
    "    output_admin_folder = \"Downloads\"\n",
    "    download_folder = os.path.join(os.path.expanduser('~'), output_admin_folder, output_folder_name)\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    return download_folder, print(\"Created the folder: \" + output_folder_name)\n",
    "\n",
    "# Get the file links\n",
    "def make_raster_link_list(links):\n",
    "    web_file_list = []\n",
    "    for link in links:\n",
    "        href = link.get('href') # the name of the file\n",
    "        if href and href.endswith('.tif') and href.startswith('treecover'):\n",
    "            web_file_list.append(href)\n",
    "    print(len(web_file_list))\n",
    "    return web_file_list\n",
    "\n",
    "def download_file(file_url, file_location):\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        failed_layers = []\n",
    "        try:\n",
    "            # Here we open the link and download the file\n",
    "            with open(file_location, 'wb') as f:\n",
    "                response = requests.get(file_url)\n",
    "                f.write(response.content)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed for {file_location}. Retrying...\")\n",
    "            retries -= 1\n",
    "            if retries == 0:\n",
    "                print(f\"Failed to download {file_location} after 3 attempts. Error: {str(e)}\")\n",
    "                failed_layers.append(file_location)\n",
    "                break\n",
    "    if failed_layers: # If there are failed layers, show them\n",
    "        print(\"the next files were not downloaded\")\n",
    "        for file in failed_layers:\n",
    "            print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_name = \"global_forest_watch_data\"\n",
    "# Create the folder\n",
    "download_folder = set_output_folder(output_folder_name)\n",
    "\n",
    "if not links:\n",
    "     # The input is expected to  be a set of links\n",
    "     # Extract tifs using regular expressions\n",
    "     links = re.findall(r'(https?://[^\\s]+\\.(?:tif|tiff))', response.text) # s?: 's' character optional due to the ? quantifier. / [^\\s]: Any character except whitespace (\\s) / + quantifier ensures that there is at least one non-whitespace character.\n",
    "     for tif_url in links:\n",
    "          filename = os.path.basename(tif_url)\n",
    "          file_location = os.path.join(download_folder, filename) # location of the download\n",
    "          tif_response = requests.get(tif_url)\n",
    "          print(\"Downloading: {}\".format(filename))\n",
    "          download_file(tif_url, file_location)\n",
    "else:\n",
    "     # Download each treecover*.tif file into the created folder\n",
    "    for link in links:\n",
    "        href = link.get('href')  # the name of the file \"kjdfnfosdjf.tif\"\n",
    "        if href and href.endswith('.tif') and href.startswith('treecover'):\n",
    "            file_location = os.path.join(download_folder, href) # location of the download\n",
    "            file_url = url + href  # We build the sentence\n",
    "            print(\"Downloading: \", file_location)\n",
    "            download_file(file_url, file_location)\n",
    "\n",
    "print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_name = \"global_forest_watch_data\"\n",
    "# Create the folder\n",
    "download_folder = set_output_folder(output_folder_name)\n",
    "# Download each treecover*.tif file into the created folder\n",
    "for link in links:\n",
    "    href = link.get('href')  # the name of the file \"kjdfnfosdjf.tif\"\n",
    "    if href and href.endswith('.tif') and href.startswith('treecover'):\n",
    "        file_name = os.path.join(download_folder, href)\n",
    "        file_url = url + href  # We build the sentence\n",
    "        print(\"Downloading: \", file_name)\n",
    "        download_file(file_url, file_name)\n",
    "\n",
    "print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check for missing files\"\"\"\n",
    "web_file_list = make_raster_link_list(links)\n",
    "# List all files in the download folder\n",
    "local_file_list = os.listdir(download_folder)\n",
    "\n",
    "# Convert both lists to sets for efficient comparison. Set eveything to minus, the files are different in naming.\n",
    "downloaded_files_set = set(map(str.lower, local_file_list))\n",
    "website_files_set = set(map(str.lower, web_file_list))\n",
    "\n",
    "# Find missing files by taking the difference between the sets\n",
    "missing_files = website_files_set - downloaded_files_set\n",
    "\n",
    "if missing_files:\n",
    "    print(\"The following files are missing:\")\n",
    "    for file in missing_files:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following files are missing:\n",
      "treecover2010_20n_120e.tif\n",
      "treecover2010_20s_010e.tif\n",
      "treecover2010_20n_110w.tif\n",
      "treecover2010_20n_110e.tif\n",
      "treecover2010_20n_160w.tif\n",
      "Missing files downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Download missing files\"\"\"\n",
    "for missing_file in missing_files:\n",
    "    layer_count = 1\n",
    "    file_url = url + missing_file\n",
    "    file_name = os.path.join(download_folder, missing_file)\n",
    "    print(\"Downloading: \", file_name, \"{} out of {}\".format(layer_count, len(missing_files)))\n",
    "    download_file(file_url, file_name)\n",
    "    layer_count += 1\n",
    "    \n",
    "print(\"Missing files downloaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_forge_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
